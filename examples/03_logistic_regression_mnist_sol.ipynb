{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 s, sys: 471 ms, total: 1.5 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('mnist', one_hot=True) \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 0.36866225187595075\n",
      "Average loss epoch 1: 0.29389054652972096\n",
      "Average loss epoch 2: 0.28459052413285196\n",
      "Average loss epoch 3: 0.2786152120375689\n",
      "Average loss epoch 4: 0.2760388553316221\n",
      "Average loss epoch 5: 0.2703886969766139\n",
      "Average loss epoch 6: 0.27216835420726343\n",
      "Average loss epoch 7: 0.2683459976058462\n",
      "Average loss epoch 8: 0.26839722262256904\n",
      "Average loss epoch 9: 0.2640930045665283\n",
      "Average loss epoch 10: 0.2609956996702092\n",
      "Average loss epoch 11: 0.26249130163993034\n",
      "Average loss epoch 12: 0.2583925482276436\n",
      "Average loss epoch 13: 0.2613478117125296\n",
      "Average loss epoch 14: 0.26050925671637476\n",
      "Average loss epoch 15: 0.2590005041245536\n",
      "Average loss epoch 16: 0.25804797573870436\n",
      "Average loss epoch 17: 0.25979868369502623\n",
      "Average loss epoch 18: 0.2598350136812195\n",
      "Average loss epoch 19: 0.2549418988691899\n",
      "Average loss epoch 20: 0.2555052127107318\n",
      "Average loss epoch 21: 0.25331701692584513\n",
      "Average loss epoch 22: 0.25600519515834486\n",
      "Average loss epoch 23: 0.2560586021791944\n",
      "Average loss epoch 24: 0.25724306007921\n",
      "Average loss epoch 25: 0.2549542024002209\n",
      "Average loss epoch 26: 0.2550519534011126\n",
      "Average loss epoch 27: 0.2512900021387425\n",
      "Average loss epoch 28: 0.25162468187180986\n",
      "Average loss epoch 29: 0.25351580463367185\n",
      "Total time: 17.801193237304688 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9179\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 1: Assemble graph\n",
    "1. Define placeholders for input and output\n",
    "2. Define the weights\n",
    "3. Define the inference model\n",
    "4. Define loss function\n",
    "5. Define optimizer\n",
    "\"\"\"\n",
    "\"\"\" Simple logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "Author: Chip Huyen\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "cs20si.stanford.edu\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.int32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\") # brodcast to all batches \n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./graphs/logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tpreds = tf.nn.softmax(logits)\n",
    "\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "\t\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\t\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\taccuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\ttotal_correct_preds += accuracy_batch[0]\t\n",
    "\t\n",
    "\tprint('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int32, shape=[2], name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(1)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
